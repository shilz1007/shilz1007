{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1e1YyoZltHnc_4fvnm5kYIT00CpcHldSk",
      "authorship_tag": "ABX9TyMmSb5ocLofxRLOr5IGfBVV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shilz1007/shilz1007/blob/main/P%2B%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Layer,Dense, Dropout, BatchNormalization,MaxPool1D\n"
      ],
      "metadata": {
        "id": "hSv9gOnR82p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def square_distance(src,dst):\n",
        "  B,N,_ = src.shape\n",
        "  _,M,_ = dst.shape\n",
        "  dist = -2 * tf.lingalg.matmul(src,dst.tf.transpose(0,2,1))\n",
        "  dist += tf.math.reduce_sum(src ** 2, -1).tf.reshape(B,N,1)\n",
        "  dist += tf.math.reduce_sum(dst ** 2, -1).tf.reshape(B,1,M)\n",
        "  return dist"
      ],
      "metadata": {
        "id": "tz5NQ4iQ9XUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import int64\n",
        "def index_points(points,idx):\n",
        "  B = points.shape[0]\n",
        "  view_shape = list(idx.shape)\n",
        "  view_shape[1:] = [1] * (len(view_shape) - 1)\n",
        "  repeat_shape = list(idx.shape)\n",
        "  repeat_shape[0] = 1\n",
        "  batch_indices = tf.range(B,dtype=int64).tf.reshape(view_shape).repeat(repeat_shape)\n",
        "  new_points = points[batch_indices, idx, :]\n",
        "  return new_points "
      ],
      "metadata": {
        "id": "mvGUYARfFADM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU7fjiAmMUzk"
      },
      "outputs": [],
      "source": [
        "def farthest_point_sample(xyz,npoint):\n",
        "  N,D = xyz.shape\n",
        "  centroids = np.zeroes((npoint,))\n",
        "  distance =  np.ones((N,)) * 1e10\n",
        "  farthest = np.random.randint(0,N)\n",
        "  for i in range(npoint):\n",
        "    centroids[i] = farthest\n",
        "    centroid = xyz[farthest,:]\n",
        "    dist = np.sum((xyz - centroid)** 2,1)\n",
        "    mask = dist < distance\n",
        "    distance[mask] = dist[mask]\n",
        "    farthest = np.argmax(distance,-1)\n",
        "  point = point[centroids.astype(np.int32)]\n",
        "  return point  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def query_ball_point(radius,nsample,xyz,new_xyz):\n",
        "  B,N,C = xyz.shape\n",
        "  _,S,_ = new_xyz.shape\n",
        "  group_idx = tf.range(N,dtype=int64).tf.reshape(1,1,N).tf.repeat([B, S, 1])\n",
        "  sqrdists = square_distance(new_xyz,xyz)\n",
        "  group_idx[sqrdists > radius ** 2] = N\n",
        "  group_idx = group_idx.tf.sort(dim=-1)[0][:,:,nsample]\n",
        "  group_first = group_idx[:,:,0].tf.reshape(B,S,1).repeat([1,1,nsample])\n",
        "  mask = group_idx == N\n",
        "  return group_idx "
      ],
      "metadata": {
        "id": "TbbQNvZTTBFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_and_group(npoint,radius,nsample,xyz,points,knn=False,use_xyz=True):\n",
        "  \n",
        "  new_xyz = index_points(xyz,farthest_point_sample(npoint,xyz))\n",
        "  \n",
        "  idx,pts_cnt = query_ball_point(npoint,nsample,xyz,new_xyz)\n",
        "\n",
        "  grouped_xyz = index_points(xyz,idx)\n",
        "  grouped_xyz -= tf.tile(tf.expand_dims(new_xyz,2),[1,1,nsample,1]) \n",
        "  if points is not None:\n",
        "     grouped_points = index_points(points,idx)\n",
        "     if use_xyz:\n",
        "       new_points = tf.concat([grouped_xyz,grouped_xyz],axis=-1)\n",
        "     else:\n",
        "       new_points = grouped_xyz\n",
        "  else:\n",
        "       new_points = grouped_xyz\n",
        "\n",
        "  return new_xyz,new_points,idx,grouped_xyz            \n",
        "     \n",
        "   "
      ],
      "metadata": {
        "id": "pkvdRouHjcsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_and_group_all(xyz,points,use_xyz=True):\n",
        "\n",
        "  batch_size = xyz.get_shape()[0]\n",
        "  nsample = xyz.get_shape()[1]\n",
        "\n",
        "  new_xyz = tf.constant(np.tile(np.array([0,0,0]).reshape((1,1,3)),(batch_size,1,1)),dtype=tf.float32)\n",
        "\n",
        "  idx = tf.constant(np.tile(np.array(range(nsample)).reshape((1,1,nsample)), (batch_size,1,1)))\n",
        "  grouped_xyz = tf.reshape(xyz,(batch_size, 1, nsample, 3))\n",
        "\n",
        "  if points is not None:\n",
        "     if use_xyz:\n",
        "        new_points = tf.concat([xyz,points])\n",
        "     else:\n",
        "        new_points = points\n",
        "     new_points = tf.expand_dims(new_points,1)\n",
        "  else:\n",
        "     new_points = grouped_xyz\n",
        "  return new_xyz, new_points,idx,grouped_xyz            \n",
        "\n"
      ],
      "metadata": {
        "id": "_nq6l9Z-q0sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2d(Layer):\n",
        "  def __init__(self,filters,strides=[1,1],activation=tf.nn.relu,padding='Valid',initializer='glorot_normal',bn=False):\n",
        "    super(Conv2d,self).__init__\n",
        "\n",
        "    self.filters = filters\n",
        "    self.strides = strides\n",
        "    self.activation = activation\n",
        "    self.padding = padding \n",
        "    self.initializer = initializer\n",
        "    self.bn = bn\n",
        "\n",
        "  def build(self,input_shape):\n",
        "\n",
        "    self.w = self.add_weight(shape=(1, 1, input_shape[-1], self.filters),initializer=self.initializer,trainable=True,name='pnet_conv')\n",
        "\n",
        "    if self.bn: self.bn_layer = BatchNormalization()\n",
        "    \n",
        "    super(Conv2d, self).build(input_shape)\n",
        "\n",
        "  def call(self,inputs,training=True):\n",
        "\n",
        "    points = tf.nn.conv2d(inputs,filters=self.w,strides=self.strides,padding=self.padding)\n",
        "\n",
        "    if self.bn: points = self.bn_layers(points,training=training)\n",
        "    if self.activation: points = self.activation(points)\n",
        "\n",
        "    return points  \n"
      ],
      "metadata": {
        "id": "DKG1VAbdfdZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pointnet_SA(Layer):\n",
        "  def __init__(\n",
        "\t\t  self, npoint, radius, nsample, mlp, group_all=False, knn=False, use_xyz=True, activation=tf.nn.relu, bn=False):\n",
        "      super(Pointnet_SA, self).__init__()\n",
        "\n",
        "      self.npoint = npoint\n",
        "      self.radius = radius\n",
        "      self.nsample = nsample\n",
        "      self.mlp = mlp\n",
        "      self.group_all = group_all\n",
        "      self.knn = knn\n",
        "      self.use_xyz = xyz \n",
        "      self.activation = activation\n",
        "      self.bn = bn\n",
        "\n",
        "      self.mlp_list = []\n",
        "\n",
        "  def build(self,input_shape):\n",
        "     for i,n_filters in enumerate(self.mlp):\n",
        "        self.mlp_list.append(Conv2d(n_filters,activation = self.activation,bn = self.bn))\n",
        "\n",
        "     super(Pointnet_SA,self).build(input_shape)\n",
        "\n",
        "  def call(self,xyz,points,training=True):\n",
        "    if points is not None:\n",
        "      if len(points.shape) < 3:\n",
        "        points = tf.expand_dims(points,axis=0)\n",
        "\n",
        "    if self.group_all:\n",
        "       nsample = xyz.get_shape()[1]\n",
        "       new_xyz, new_points,idx,grouped_xyz = sample_and_group_all(xyz,points,self.use_xyz)\n",
        "\n",
        "    else:\n",
        "      new_xyz, new_points,idx, grouped_xyz = sample_and_group(\n",
        "                                             self.npoint,\n",
        "                                             self.radius,\n",
        "                                             self.nsample,\n",
        "                                             xyz,\n",
        "                                             points,\n",
        "                                             self.knn,\n",
        "                                             use_xyz = self.use_xyz)\n",
        "\n",
        "    for i, mlp_layer in enumerate(self.mlp_list):\n",
        "        new_points = mlp_layer(new_points,training=training)\n",
        "\n",
        "    new_points = tf.math.reduce_max(new_points,axis=2,keepdims=True)\n",
        "\n",
        "    return new_xyz,tf.squeeze(new_points)                        \n",
        "\n",
        "\t\t  \n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\t   "
      ],
      "metadata": {
        "id": "8icPtD8-bQ44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pointnet_SA_MSG(Layer):\n",
        "\n",
        "  def __init__(self,npoint,radius_list,nsample_list,mlp,use_xyz=True,activation=tf.nn.relu,bn=False):\n",
        "      super(Pointnet_SA_MSG,self)._init_()\n",
        "      self.npoint = npoint\n",
        "      self.radius_list = radius_list\n",
        "      self.nsample_list = nsample_list\n",
        "      self.mlp = mlp\n",
        "      self.use_xyz = use_xyz\n",
        "      self.activation = activation\n",
        "      self.bn = bn\n",
        "\n",
        "      self.mlp_list = []\n",
        "\n",
        "  def build(self,input_shape):\n",
        "\n",
        "    for i in range(len(self.radius_list)):\n",
        "        temp_list = []\n",
        "        for i , n_filters in enumerate(self.mlp[i]):\n",
        "            temp_list.append(Conv2d(n_filters,activation=self.activation,bn=self.bn))\n",
        "        self.mlp_list.append(temp_list)\n",
        "    super(Pointnet_SA_MSG,self).build(input_shape)\n",
        "\n",
        "  def Call(self,xyz,points, training=True):\n",
        "    if points is not None:\n",
        "      if len(points.shape) < 3:\n",
        "         points = tf.expand_dims(points,axis = 0)\n",
        "\n",
        "    new_xyz = index_points(xyz,farthest_point_sample(self.npoint,xyz))\n",
        "\n",
        "    new_point_list = []\n",
        "\n",
        "    for i in range(len(self.radius_list)):\n",
        "        radius = self.radius_list[i]\n",
        "        nsample = self.nsample[i]\n",
        "        idx,pts_cnt = query_ball_point(radius,nsample,xyz,new_xyz)\n",
        "        grouped_xyz = index_points(xyz,idx)\n",
        "        grouped_xyz -= tf.tile(tf.expand_dims(new_xyz, 2), [1,1,nsample,1])\n",
        "\n",
        "        if points is not None:\n",
        "          grouped_points = index_points(points,idx)\n",
        "          if self.use_xyz:\n",
        "            grouped_points = tf.concat([grouped_points,grouped_xyz],axis=-1)\n",
        "        else:\n",
        "          grouped_points = grouped_xyz\n",
        "\n",
        "        for i,mlp_layer in enumerate(self.mlp_list[i]):\n",
        "          grouped_points = mlp_layer(grouped_points,trainig = training)\n",
        "\n",
        "        new_points = tf.math.reduce_max(grouped_points,axis = 2)\n",
        "        new_point_list.append(new_points)  \n",
        "\n",
        "    new_points_concat = tf.concat(new_point_list,axis = -1)\n",
        "\n",
        "    return new_xyz, new_points_concat            \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            \n"
      ],
      "metadata": {
        "id": "jXmIqb8zIZWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLS_MSG_Model(Model):\n",
        "   def __init__(self, batch_size, num_classes, bn=False, activation=tf.nn.relu):\n",
        "     super(CLS_MSG_Model, self).__init__()\n",
        "\n",
        "     self.activation = activation\n",
        "     self.batch_size = batch_size\n",
        "     self.num_classes = num_classes\n",
        "     self.bn = bn\n",
        "\n",
        "     self.kernel_initializer = 'glorot_normal'\n",
        "     self.kernel_regularizer = None\n",
        "\n",
        "     self.init_network()\n",
        "\n",
        "   def init_network(self):\n",
        "\n",
        "     self.layer1 = Pointnet_SA_MSG(\n",
        "                npoint=1024,\n",
        "                radius_list=[0.1,0.2,0.4],\n",
        "                nsample_list=[16,32,128],\n",
        "                mlp=[[32,32,64], [64,64,128], [64,96,128]],\n",
        "                activation=self.activation,\n",
        "                bn = self.bn\n",
        "                )\n",
        "     self.layer2 = Pointnet_SA_MSG(\n",
        "                npoint=512,\n",
        "                radius_list=[0.2,0.4,0.8],\n",
        "                nsample_list=[32,64,128],\n",
        "                mlp=[[64,64,128], [128,128,256], [128,128,256]],\n",
        "                activation=self.activation,\n",
        "                bn = self.bn\n",
        "                )\n",
        "     self.layer3 = Pointnet_SA(npoint=None,\n",
        "                               radius=None,\n",
        "                               nsample=None,\n",
        "                               mlp=[256, 512, 1024],\n",
        "                               group_all=True,\n",
        "                               activation=self.activation,\n",
        "                               bn = self.bn)\n",
        "     self.dense1 = Dense(512, activation=self.activation)\n",
        "     self.dropout1 = Dropout(self.keep_prob)\n",
        "     self.dense2 = Dense(128, activation=self.activation)\n",
        "     self.dense2 = Dense(128, activation=self.activation)\n",
        "\n",
        "   def forward_pass(self,input,training):\n",
        "\n",
        "    xyz, points = self.layer1(input, None, training=training)\n",
        "    xyz, points = self.layer2(xyz, points, training=training)\n",
        "    xyz, points = self.layer3(xyz, points, training=training)\n",
        "\n",
        "    net = tf.reshape(points, (self.batch_size, -1))\n",
        "    net = self.dense1(net)\n",
        "    net = self.dropout1(net)\n",
        "    net = self.dense2(net)\n",
        "    net = self.dropout2(net)\n",
        "    pred = self.dense3(net)\n",
        "\n",
        "    return pred\n",
        "\n",
        "   def train_step(self,input):\n",
        "     with tf.GradientTape() as tape:\n",
        "         pred = self.forward_pass(input[0], True)\n",
        "         loss = self.compiled_loss(input[1], pred)\n",
        "     gradients = tape.gradient(loss, self.trainable_variables)\n",
        "     self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "     self.compiled_metrics.update_state(input[1], pred)\n",
        "     return {m.name: m.result() for m in self.metrics}  \n",
        "\n",
        "   def test_step(self,input):\n",
        "     pred = self.forward_pass(input[0], False)\n",
        "     loss = self.compiled_loss(input[1], pred)\n",
        "\n",
        "     self.compiled_metrics.update_state(input[1], pred)\n",
        "     return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "   def call(self,input,training=False):\n",
        "      return self.forward_pass(input, training)        "
      ],
      "metadata": {
        "id": "l5eUr-e6XXYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "npoints = 200\n",
        "N = 5\n",
        "centroids = np.zeros((npoints,0))\n",
        "print(centroids)\n",
        "distance = np.ones((N,)) * 1e5\n",
        "print(distance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VJ8PR3PNwkO",
        "outputId": "077748a7-4aef-4094-ab62-5aa9899412d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[100000. 100000. 100000. 100000. 100000.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SQig8uE_NnAS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}