{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1dJk2-8cye8HRvMN6YfI7a0xC51JjnVQ3",
      "authorship_tag": "ABX9TyNlnWJ1VnxIMlpi4kywsNeP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shilz1007/shilz1007/blob/main/3D_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yx6Itv3pKVWR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense , Reshape,Flatten,Conv3D,Conv3DTranspose,LeakyReLU,Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.python import metrics\n",
        "from tensorflow.keras.metrics import binary_crossentropy,categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = ('/content/drive/MyDrive/modelnet10.npz')"
      ],
      "metadata": {
        "id": "2WkIWebYKphx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load(path, allow_pickle=True)\n",
        "train_voxel = data[\"train_voxel\"] # Training 3D voxel samples\n",
        "#test_voxel = data[\"test_voxel\"] # Test 3D voxel samples\n",
        "train_labels = data[\"train_labels\"] # Training labels (integers from 0 to 9)\n",
        "#test_labels = data[\"test_labels\"] # Test labels (integers from 0 to 9)\n",
        "class_map = data[\"class_map\"] # Dictionary mapping the labels to their class names."
      ],
      "metadata": {
        "id": "QPLiq1s4KuOZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_voxel.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV_SCaoYNc_p",
        "outputId": "ac97a208-e649-45a6-a1f3-35744813f58f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3991, 64, 64, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(learning_rate=0.0002,beta_1 = 0.5)"
      ],
      "metadata": {
        "id": "sGY3_eq4Qolt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_discriminator(in_shape=(64,64,64,1)):\n",
        "  print('inside discriminator')\n",
        "  model = Sequential()\n",
        "  model.add(Conv3D(64,(4,4,4), strides=(2,2,2), padding ='same', input_shape = in_shape))\n",
        "  model.add(LeakyReLU(alpha= 0.00001))\n",
        "  model.add(BatchNormalization())\n",
        "  \n",
        "\n",
        "  model.add(Conv3D(64,(4,4,4), strides=(2,2,2), padding ='same'))\n",
        "  model.add(LeakyReLU(alpha= 0.00001))\n",
        "  model.add(BatchNormalization())\n",
        "  \n",
        "\n",
        "  model.add(Conv3D(128,(4,4,4), strides=(2,2,2), padding ='same'))\n",
        "  model.add(LeakyReLU(alpha= 0.00001))\n",
        "  model.add(BatchNormalization())\n",
        "  \n",
        "\n",
        "  model.add(Conv3D(256,(4,4,4), strides=(2,2,2), padding ='same'))\n",
        "  model.add(LeakyReLU(alpha= 0.00001))\n",
        "  model.add(BatchNormalization())\n",
        "  \n",
        "\n",
        "  model.add(Conv3D(512,(4,4,4), strides=(2,2,2), padding ='same'))\n",
        "  model.add(LeakyReLU(alpha= 0.00001))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  #model.add(Conv3D(512,(4,4,4), strides=(2,2,2), padding ='same'))\n",
        "  #model.add(LeakyReLU(alpha= 0.00001))\n",
        "  #model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.4))\n",
        "  ##assert model.output_shape == (None,4096)\n",
        "  model.add(Dense(64,activation='sigmoid'))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "  #model.add(Reshape(32,32,32))\n",
        "  model.compile(loss=binary_crossentropy,optimizer=opt, metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "FYq5vWXHIi5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_discr = define_discriminator()\n",
        "print(test_discr.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxRNkd0iQ-d4",
        "outputId": "f0afc4e5-2663-4022-9614-489d5fac3c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside discriminator\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_5 (Conv3D)           (None, 32, 32, 32, 64)    4160      \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 32, 32, 32, 64)    0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 32, 32, 32, 64)   256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv3d_6 (Conv3D)           (None, 16, 16, 16, 64)    262208    \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 16, 64)    0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 16, 16, 16, 64)   256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv3d_7 (Conv3D)           (None, 8, 8, 8, 128)      524416    \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 8, 8, 8, 128)      0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 8, 8, 8, 128)     512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv3d_8 (Conv3D)           (None, 4, 4, 4, 256)      2097408   \n",
            "                                                                 \n",
            " leaky_re_lu_8 (LeakyReLU)   (None, 4, 4, 4, 256)      0         \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 4, 4, 4, 256)     1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv3d_9 (Conv3D)           (None, 2, 2, 2, 512)      8389120   \n",
            "                                                                 \n",
            " leaky_re_lu_9 (LeakyReLU)   (None, 2, 2, 2, 512)      0         \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 2, 2, 2, 512)     2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                262208    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,543,681\n",
            "Trainable params: 11,541,633\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def define_generator(latent_dim):\n",
        "  model = Sequential()\n",
        "  n_nodes = 200*1*1*1\n",
        "  #model.add(Dense(n_nodes,input_dim = latent_dim))\n",
        "  #model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Reshape((1,1,1,200)))\n",
        "  model.add(Conv3DTranspose(512,(4,4,4), strides=(1,1,1),padding='valid'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Conv3DTranspose(256,(4,4,4), strides=(2,2,2),padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Conv3DTranspose(128,(4,4,4), strides=(2,2,2),padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Conv3DTranspose(128,(4,4,4), strides=(2,2,2),padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Conv3DTranspose(64,(4,4,4), strides=(2,2,2),padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  #model.add(Conv3DTranspose(128,(4,4,4), strides=(2,2,2),padding='same'))\n",
        "  #model.add(BatchNormalization())\n",
        "  #model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Conv3D(1,(8,8,8),activation = 'sigmoid',padding = 'same'))\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "HGz8KsQTTc4p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_gen = define_generator(200)\n",
        "print(test_gen.summary())"
      ],
      "metadata": {
        "id": "VH-77VgQRMY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_latent_points(latent_dim,n_samples):\n",
        "  X_input = np.random.randn(latent_dim * n_samples)\n",
        "  X_input = X_input.reshape(n_samples,latent_dim)\n",
        "  return X_input  "
      ],
      "metadata": {
        "id": "ftRGL7JeDWnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_gan(generator,discriminator):\n",
        "  discriminator.trainable = False\n",
        "  model = Sequential()\n",
        "  model.add(generator)\n",
        "  model.add(discriminator)\n",
        "  opt = Adam(learning_rate = 0.0025,beta_1 = 0.5)\n",
        "  model.compile(loss='binary_crossentropy',optimizer = opt)\n",
        "  return model"
      ],
      "metadata": {
        "id": "Za-oSfrFlPFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_real_samples(train_voxel,n_samples):\n",
        "  #choose random images\n",
        "  ix = np.random.randint(0,train_voxel.shape[0],n_samples)\n",
        "  X = train_voxel[ix]\n",
        "  #assigning 1 to y indicating real images\n",
        "  y = np.ones((n_samples,1))\n",
        "  return X,y"
      ],
      "metadata": {
        "id": "kP8I5naB0sWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fake_Samples(generator,latent_dim,n_samples):\n",
        "  #generate points in latent space\n",
        "  x_input = generate_latent_points(latent_dim,n_samples)\n",
        "  #predict using generator to generate fake samples\n",
        "  X = generator.predict(x_input)\n",
        "  # labeled as 0 since they are fake samples\n",
        "  y = np.zeros((n_samples,1))\n",
        "  return X,y\n"
      ],
      "metadata": {
        "id": "OExqjvHPmnC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(g_model, d_model,gan_model,train_voxel,latent_dim,n_epochs = 3, n_batch= 10):\n",
        "  bat_per_epo = int(train_voxel.shape[0] / n_batch)\n",
        "  half_batch = int(n_batch / 2)\n",
        "\n",
        "  #training loop\n",
        "  for i in range(n_epochs):\n",
        "    for j in range(n_batch):\n",
        "      \n",
        "      X_real,y_real = get_real_samples(train_voxel,half_batch)\n",
        "      #measuring loss of discriminator\n",
        "      d_loss_real,_ = d_model.train_on_batch(X_real,y_real) \n",
        "\n",
        "      #generate fake samples\n",
        "      X_fake,y_fake = generate_fake_Samples(g_model,latent_dim,half_batch)\n",
        "      d_loss_fake,_ = d_model.train_on_batch(X_fake,y_fake)\n",
        "\n",
        "      #prepare latent points as input to the generator\n",
        "      X_gan = generate_latent_points(latent_dim,n_batch)\n",
        "\n",
        "      #generated samples are labeled as valid to fool the discriminator\n",
        "      y_gan = np.ones((n_batch,1))\n",
        "\n",
        "      #train generator with latent_dim and y as 1\n",
        "      g_loss = gan_model.train_on_batch(X_gan,y_gan)\n",
        "\n",
        "      print('Epoch>%d, Batch %d/%d,d1=%.3f,d2=%.3f,g=%.3f' %\n",
        "            (i+1,j+1,bat_per_epo,d_loss_real,d_loss_fake,g_loss))"
      ],
      "metadata": {
        "id": "IScMxSVtOzQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train the gan\n",
        "#size of latent space\n",
        "latent_dim = 200\n",
        "#create the discriminator\n",
        "discriminator = define_discriminator()\n",
        "#create the generator\n",
        "generator = define_generator(latent_dim)\n",
        "#create gan\n",
        "gan_model = define_gan(generator,discriminator)\n",
        "#train model\n",
        "train(generator,discriminator,gan_model,train_voxel,latent_dim,n_epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V68HwuC53HA",
        "outputId": "4c028fac-4c80-454f-de80-e0f07f7dc5b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside discriminator\n",
            "Epoch>1, Batch 1/399,d1=0.897,d2=0.240,g=0.924\n",
            "Epoch>1, Batch 2/399,d1=0.169,d2=0.396,g=0.931\n",
            "Epoch>1, Batch 3/399,d1=0.138,d2=0.008,g=0.934\n",
            "Epoch>1, Batch 4/399,d1=0.194,d2=0.008,g=0.907\n",
            "Epoch>1, Batch 5/399,d1=0.080,d2=0.008,g=0.902\n",
            "Epoch>1, Batch 6/399,d1=0.059,d2=0.007,g=0.900\n",
            "Epoch>1, Batch 7/399,d1=0.045,d2=0.007,g=0.911\n",
            "Epoch>1, Batch 8/399,d1=0.038,d2=0.007,g=0.906\n",
            "Epoch>1, Batch 9/399,d1=0.034,d2=0.007,g=0.906\n",
            "Epoch>1, Batch 10/399,d1=0.029,d2=0.007,g=0.909\n",
            "Epoch>2, Batch 1/399,d1=0.023,d2=0.007,g=0.921\n",
            "Epoch>2, Batch 2/399,d1=0.023,d2=0.007,g=0.919\n",
            "Epoch>2, Batch 3/399,d1=0.032,d2=0.007,g=0.927\n",
            "Epoch>2, Batch 4/399,d1=0.022,d2=0.007,g=0.923\n",
            "Epoch>2, Batch 5/399,d1=0.045,d2=0.007,g=0.942\n",
            "Epoch>2, Batch 6/399,d1=0.018,d2=0.007,g=0.947\n",
            "Epoch>2, Batch 7/399,d1=0.017,d2=0.007,g=0.952\n",
            "Epoch>2, Batch 8/399,d1=0.226,d2=0.007,g=0.946\n",
            "Epoch>2, Batch 9/399,d1=0.044,d2=0.007,g=0.948\n",
            "Epoch>2, Batch 10/399,d1=0.035,d2=0.007,g=0.946\n",
            "Epoch>3, Batch 1/399,d1=0.040,d2=0.007,g=0.970\n",
            "Epoch>3, Batch 2/399,d1=0.024,d2=0.007,g=0.963\n",
            "Epoch>3, Batch 3/399,d1=0.018,d2=0.007,g=0.981\n",
            "Epoch>3, Batch 4/399,d1=0.017,d2=0.006,g=0.983\n",
            "Epoch>3, Batch 5/399,d1=0.018,d2=0.007,g=0.988\n",
            "Epoch>3, Batch 6/399,d1=0.018,d2=0.006,g=0.994\n",
            "Epoch>3, Batch 7/399,d1=0.039,d2=0.006,g=1.020\n",
            "Epoch>3, Batch 8/399,d1=0.018,d2=0.006,g=1.019\n",
            "Epoch>3, Batch 9/399,d1=0.023,d2=0.006,g=1.050\n",
            "Epoch>3, Batch 10/399,d1=0.017,d2=0.006,g=1.039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "discriminator.save('/content/drive/MyDrive/3D-discriminator.h5')"
      ],
      "metadata": {
        "id": "zzTSS_nQqbky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.save('/content/drive/MyDrive/3D-generator.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjPXJ0xarfnI",
        "outputId": "fbf25383-70a9-4d1d-95e7-ea685d66630b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "temp = load_model('/content/drive/MyDrive/3D-discriminator.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFYZv3wArzjw",
        "outputId": "cb59075b-0161-43d7-91ec-aa94c2b25e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcIaQ8gxsIlX",
        "outputId": "da555551-4142-4a4b-b055-75cbd95d81b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_11 (Conv3D)          (None, 32, 32, 32, 64)    4160      \n",
            "                                                                 \n",
            " leaky_re_lu_15 (LeakyReLU)  (None, 32, 32, 32, 64)    0         \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 32, 32, 32, 64)   256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv3d_12 (Conv3D)          (None, 16, 16, 16, 64)    262208    \n",
            "                                                                 \n",
            " leaky_re_lu_16 (LeakyReLU)  (None, 16, 16, 16, 64)    0         \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 16, 16, 16, 64)   256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv3d_13 (Conv3D)          (None, 8, 8, 8, 128)      524416    \n",
            "                                                                 \n",
            " leaky_re_lu_17 (LeakyReLU)  (None, 8, 8, 8, 128)      0         \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 8, 8, 8, 128)     512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv3d_14 (Conv3D)          (None, 4, 4, 4, 256)      2097408   \n",
            "                                                                 \n",
            " leaky_re_lu_18 (LeakyReLU)  (None, 4, 4, 4, 256)      0         \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 4, 4, 4, 256)     1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv3d_15 (Conv3D)          (None, 2, 2, 2, 512)      8389120   \n",
            "                                                                 \n",
            " leaky_re_lu_19 (LeakyReLU)  (None, 2, 2, 2, 512)      0         \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 2, 2, 2, 512)     2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                262208    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,543,681\n",
            "Trainable params: 0\n",
            "Non-trainable params: 11,543,681\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp.get_weights()"
      ],
      "metadata": {
        "id": "u8pL-4UbtMQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp.optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5phhmpMAtkmz",
        "outputId": "8068ec61-368c-4dcc-d189-bef1a9f28b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.optimizer_v2.adam.Adam at 0x7fe7ca01d810>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layers = temp.layers\n",
        "print(layers[0])\n",
        "#filters,biases = layers_classifier[1].get_weights()\n",
        "#print(layers_classifier[1],filters.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK7ax61wtpzG",
        "outputId": "0bfc167d-8f7b-4426-dc69-8e77cd09b91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.layers.convolutional.Conv3D object at 0x7fd320074690>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import MaxPooling3D\n",
        "opt = Adam(learning_rate=0.0002,beta_1 = 0.5)"
      ],
      "metadata": {
        "id": "c7uPepRO6eEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_classifier(layers):\n",
        "\n",
        "  conv1 = layers[5].output\n",
        "  out1 = tf.keras.layers.MaxPool3D(pool_size=(8,8,8))(conv1)\n",
        "  conv2 = layers[8].output\n",
        "  out2 = tf.keras.layers.MaxPool3D(pool_size=(4,4,4))(conv2)\n",
        "  conv3 = layers[11].output\n",
        "  out3 = tf.keras.layers.MaxPool3D(pool_size=(2,2,2))(conv3)\n",
        "  conc = tf.keras.layers.Concatenate()([out1,out2,out3])\n",
        "  flat = tf.keras.layers.Flatten()(conc)\n",
        "  output = tf.keras.layers.Dense(10,activation='softmax')(flat)\n",
        "  model = tf.keras.Model(inputs = conv1,outputs = output)\n",
        "  model.compile(loss=categorical_crossentropy,optimizer=opt, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0bYjO37P2KR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = define_classifier(layers)\n",
        "print(classifier.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoaDzyYF7rf0",
        "outputId": "9c2f33a4-caf7-46a0-9c24-cea1104c6be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 16, 16, 16,  0           []                               \n",
            "                                 64)]                                                             \n",
            "                                                                                                  \n",
            " conv3d_13 (Conv3D)             (None, 8, 8, 8, 128  524416      ['input_4[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " leaky_re_lu_17 (LeakyReLU)     (None, 8, 8, 8, 128  0           ['conv3d_13[4][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 8, 8, 8, 128  512        ['leaky_re_lu_17[4][0]']         \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " conv3d_14 (Conv3D)             (None, 4, 4, 4, 256  2097408     ['batch_normalization_17[4][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " leaky_re_lu_18 (LeakyReLU)     (None, 4, 4, 4, 256  0           ['conv3d_14[4][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 4, 4, 4, 256  1024       ['leaky_re_lu_18[4][0]']         \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling3d_12 (MaxPooling3D  (None, 2, 2, 2, 64)  0          ['input_4[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling3d_13 (MaxPooling3D  (None, 2, 2, 2, 128  0          ['batch_normalization_17[4][0]'] \n",
            " )                              )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling3d_14 (MaxPooling3D  (None, 2, 2, 2, 256  0          ['batch_normalization_18[4][0]'] \n",
            " )                              )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 2, 2, 2, 448  0           ['max_pooling3d_12[1][0]',       \n",
            "                                )                                 'max_pooling3d_13[1][0]',       \n",
            "                                                                  'max_pooling3d_14[1][0]']       \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 3584)         0           ['concatenate_4[1][0]']          \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 10)           35850       ['flatten_4[1][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,659,210\n",
            "Trainable params: 35,850\n",
            "Non-trainable params: 2,623,360\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(classifier,train_voxel,num_epochs=2,num_batch=10):\n",
        "  for i in range(num_epochs):\n",
        "    for j in range(num_batch):\n",
        "      C_loss,_ = classifier.train_on_batch(train_voxel)\n",
        "      print('Epoch>%d, Batch %d/%d,d1=%.3f' %\n",
        "            (i+1,j+1,C_loss))\n"
      ],
      "metadata": {
        "id": "BexMg0yzk1AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_images(train_voxel):\n",
        "\n",
        "  #a = tf.ones(shape=[1,64,64,64,1])\n",
        "  #a.shape\n",
        "\n",
        "  a2 = tf.reshape(train_voxel,[-1,64,64,64*1])\n",
        "  a2.shape\n",
        "\n",
        "  a3 = tf.image.resize(a2,[16,16])\n",
        "  a3.shape\n",
        "\n",
        "  a4 = tf.reshape(a3,[-1,16,16,64,1])\n",
        "  a4.shape\n",
        "\n",
        "  transposed = tf.transpose(a4,[0,1,3,2,4])\n",
        "  transposed.shape\n",
        "\n",
        "  a5 = tf.reshape(transposed,[1*16,64,16,1])\n",
        "  a5.shape\n",
        "\n",
        "  new_size = tf.constant([16,16])\n",
        "  resized = tf.image.resize(a5,new_size)\n",
        "  resized.shape\n",
        "\n",
        "#undo transpose \n",
        "  undo_reshape = tf.reshape(resized,[1,16,16,16,1])\n",
        "  undo_transpose = tf.transpose(undo_reshape,[0,2,3,1,4])\n",
        "  undo_transpose.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzNW6jVUzKAG",
        "outputId": "184c25aa-fef1-42e5-886a-4d025813feae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 16, 16, 16, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_var = np.zeros((16, 16, 16))"
      ],
      "metadata": {
        "id": "PM0LpXfaHyMt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3991):\n",
        "  a2 = tf.reshape(train_voxel[i],[-1,64,64,64*1])\n",
        "  a3 = tf.image.resize(a2,[16,16])\n",
        "  a4 = tf.reshape(a3,[-1,16,16,64,1])\n",
        "  transposed = tf.transpose(a4,[0,1,3,2,4])\n",
        "  a5 = tf.reshape(transposed,[1*16,64,16,1])\n",
        "  new_size = tf.constant([16,16])\n",
        "  resized = tf.image.resize(a5,new_size)\n",
        "  undo_reshape = tf.reshape(resized,[1,16,16,16,1])\n",
        "  undo_transpose = tf.transpose(undo_reshape,[0,2,3,1,4])\n",
        "  new_var[i] = undo_transpose\n",
        "new_var[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "LU1X4oOGF9i9",
        "outputId": "7d906732-2b00-473d-8658-a54e3e4cb7d9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-5ec0dbcf0ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mundo_reshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mundo_transpose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mundo_reshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mnew_var\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mundo_transpose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mnew_var\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (16,16,16,1) into shape (16,16)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_classifier(classifier,train_voxel,num_epochs=2,num_batch=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "wAx8NilAg8SI",
        "outputId": "bc98cc12-cb04-4d96-b5c1-85af3995a387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-72e77d6ed99e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_voxel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-230813bce7de>\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(classifier, train_voxel, num_epochs, num_batch)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m       \u001b[0mC_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_voxel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m       print('Epoch>%d, Batch %d/%d,d1=%.3f' %\n\u001b[1;32m      6\u001b[0m             (i+1,j+1,C_loss))\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2091\u001b[0m                                                     class_weight)\n\u001b[1;32m   2092\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2093\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2095\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model_3\" is incompatible with the layer: expected shape=(None, 16, 16, 16, 64), found shape=(3991, 64, 64, 64)\n"
          ]
        }
      ]
    }
  ]
}