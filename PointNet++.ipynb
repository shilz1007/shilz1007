{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shilz1007/shilz1007/blob/main/PointNet%2B%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSv9gOnR82p4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import trimesh\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers,Model,activations\n",
        "from tensorflow.keras.layers import Layer,Dense, Dropout, BatchNormalization,MaxPool1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import Regularizer\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "tf.random.set_seed(1234)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9xDNbCBBjVG",
        "outputId": "1c09ecaf-12b2-4085-c8f4-03a3ecfb9734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting trimesh\n",
            "  Downloading trimesh-3.16.2-py3-none-any.whl (663 kB)\n",
            "\u001b[K     |████████████████████████████████| 663 kB 15.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from trimesh) (1.21.6)\n",
            "Installing collected packages: trimesh\n",
            "Successfully installed trimesh-3.16.2\n"
          ]
        }
      ],
      "source": [
        "!pip install trimesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRfmifms-1ei",
        "outputId": "3290b551-d773-4acb-b0ac-1b6c5170c1ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\n",
            "473402300/473402300 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "Data_ModelNet = tf.keras.utils.get_file(\n",
        "    \"modelnet.zip\",\n",
        "    \"http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\",\n",
        "    extract=True,\n",
        ")                        \n",
        "Data_ModelNet = os.path.join(os.path.dirname(Data_ModelNet), \"ModelNet10\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTTQpFsz-4WG"
      },
      "outputs": [],
      "source": [
        "mesh = trimesh.load(os.path.join(Data_ModelNet,\"chair/train/chair_0370.off\"))\n",
        "mesh.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJ0UMRzH-5G_"
      },
      "outputs": [],
      "source": [
        "points = mesh.sample(2048)\n",
        "fig = plt.figure(figsize=(5,5))\n",
        "ax = fig.add_subplot(111, projection=\"3d\")\n",
        "ax.scatter(points[:, 0], points[:, 1], points[:, 2])\n",
        "ax.set_axis_off()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSonAf6aCRsY"
      },
      "outputs": [],
      "source": [
        "def create_test_train(num_points=2048):\n",
        "  print('inside')\n",
        "  train_points = []\n",
        "  train_labels = []\n",
        "  test_points  = []\n",
        "  test_labels  = []\n",
        "  class_map = {}\n",
        "  folders = glob.glob(os.path.join(Data_ModelNet,\"[!README]*\"))\n",
        "  #print(folders)\n",
        "\n",
        "  for i, folder in enumerate(folders):\n",
        "    #print(\"processing class: {}\".format(os.path.basename(folder)))\n",
        "    class_map[i] = folder.split(\"/\")[-1]\n",
        "    #print(class_map[i])\n",
        "    train_files = glob.glob(os.path.join(folder,\"train/*\"))\n",
        "    test_files = glob.glob(os.path.join(folder,\"test/*\"))\n",
        "\n",
        "    for f in train_files:\n",
        "      train_points.append(trimesh.load(f).sample(num_points))\n",
        "      train_labels.append(i)\n",
        "\n",
        "    for f in test_files:\n",
        "      test_points.append(trimesh.load(f).sample(num_points))\n",
        "      test_labels.append(i)\n",
        "\n",
        "  print(class_map)\n",
        "  return(\n",
        "      np.array(train_points),\n",
        "      np.array(test_points),\n",
        "      np.array(train_labels),\n",
        "      np.array(test_labels),\n",
        "      class_map,\n",
        "  )     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cRdN5ke-8rG",
        "outputId": "86d9b796-1dc6-4eba-9d6a-6c2722fb6d54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inside\n"
          ]
        }
      ],
      "source": [
        "points = 2048\n",
        "classes = 10\n",
        "batch_size = 32\n",
        "train_points, test_points, train_labels, test_labels, CLASS_MAP = create_test_train(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnZx3Qw1_Lwm",
        "outputId": "67dcf855-95c5-41bb-af40-21f7f855be11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "night_stand\n",
            "bed\n",
            "desk\n",
            "bathtub\n",
            "monitor\n",
            "table\n",
            "dresser\n",
            "chair\n",
            "toilet\n",
            "sofa\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  print(CLASS_MAP[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s3XaUsQ_UO8"
      },
      "outputs": [],
      "source": [
        "def augment(points,labels):\n",
        "  points += tf.random.uniform(points.shape,-0.005,0.005,dtype=tf.float64)\n",
        "  points = tf.random.shuffle(points)\n",
        "  return points,labels\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_points,train_labels))\n",
        "test_dataset =  tf.data.Dataset.from_tensor_slices((test_points,test_labels))\n",
        "\n",
        "train_dataset = train_dataset.shuffle(len(train_points)).map(augment).batch(batch_size)\n",
        "test_dataset = test_dataset.shuffle(len(test_points)).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz5NQ4iQ9XUz"
      },
      "outputs": [],
      "source": [
        "def square_distance(src,dst):\n",
        "  B,N,_ = src.shape\n",
        "  _,M,_ = dst.shape\n",
        "  dist = -2 * tf.lingalg.matmul(src,dst.tf.transpose(0,2,1))\n",
        "  dist += tf.math.reduce_sum(src ** 2, -1).tf.reshape(B,N,1)\n",
        "  dist += tf.math.reduce_sum(dst ** 2, -1).tf.reshape(B,1,M)\n",
        "  return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvGUYARfFADM"
      },
      "outputs": [],
      "source": [
        "from numpy import int64\n",
        "def index_points(points,idx):\n",
        "  B = points.shape[0]\n",
        "  view_shape = list(idx.shape)\n",
        "  view_shape[1:] = [1] * (len(view_shape) - 1)\n",
        "  repeat_shape = list(idx.shape)\n",
        "  repeat_shape[0] = 1\n",
        "  batch_indices = tf.range(B,dtype=int64).tf.reshape(view_shape).repeat(repeat_shape)\n",
        "  new_points = points[batch_indices, idx, :]\n",
        "  return new_points "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU7fjiAmMUzk"
      },
      "outputs": [],
      "source": [
        "def farthest_point_sample(xyz,npoint):\n",
        "  N,D = xyz.shape\n",
        "  centroids = np.zeroes((npoint,))\n",
        "  distance =  np.ones((N,)) * 1e10 \n",
        "  farthest = np.random.randint(0,N)\n",
        "  for i in range(npoint):\n",
        "    centroids[i] = farthest\n",
        "    centroid = xyz[farthest,:]\n",
        "    dist = np.sum((xyz - centroid)** 2,1)\n",
        "    mask = dist < distance\n",
        "    distance[mask] = dist[mask]\n",
        "    farthest = np.argmax(distance,-1)\n",
        "  point = point[centroids.astype(np.int32)]\n",
        "  return point  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbbQNvZTTBFX"
      },
      "outputs": [],
      "source": [
        "def query_ball_point(radius,nsample,xyz,new_xyz):\n",
        "  B,N,C = xyz.shape\n",
        "  _,S,_ = new_xyz.shape\n",
        "  group_idx = tf.range(N,dtype=int64).tf.reshape(1,1,N).tf.repeat([B, S, 1])\n",
        "  sqrdists = square_distance(new_xyz,xyz)\n",
        "  group_idx[sqrdists > radius ** 2] = N\n",
        "  group_idx = group_idx.tf.sort(dim=-1)[0][:,:,nsample]\n",
        "  group_first = group_idx[:,:,0].tf.reshape(B,S,1).repeat([1,1,nsample])\n",
        "  mask = group_idx == N\n",
        "  return group_idx "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkvdRouHjcsA"
      },
      "outputs": [],
      "source": [
        "def sample_and_group(npoint,radius,nsample,xyz,points,knn=False,use_xyz=True):\n",
        "  \n",
        "  new_xyz = index_points(xyz,farthest_point_sample(npoint,xyz))\n",
        "  \n",
        "  idx,pts_cnt = query_ball_point(npoint,nsample,xyz,new_xyz)\n",
        "\n",
        "  grouped_xyz = index_points(xyz,idx)\n",
        "  grouped_xyz -= tf.tile(tf.expand_dims(new_xyz,2),[1,1,nsample,1]) \n",
        "  if points is not None:\n",
        "     grouped_points = index_points(points,idx)\n",
        "     if use_xyz:\n",
        "       new_points = tf.concat([grouped_xyz,grouped_xyz],axis=-1)\n",
        "     else:\n",
        "       new_points = grouped_xyz\n",
        "  else:\n",
        "       new_points = grouped_xyz\n",
        "\n",
        "  return new_xyz,new_points,idx,grouped_xyz            \n",
        "     \n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nq6l9Z-q0sT"
      },
      "outputs": [],
      "source": [
        "def sample_and_group_all(xyz,points,use_xyz=True):\n",
        "\n",
        "  batch_size = xyz.get_shape()[0]\n",
        "  nsample = xyz.get_shape()[1]\n",
        "\n",
        "  new_xyz = tf.constant(np.tile(np.array([0,0,0]).reshape((1,1,3)),(batch_size,1,1)),dtype=tf.float32)\n",
        "\n",
        "  idx = tf.constant(np.tile(np.array(range(nsample)).reshape((1,1,nsample)), (batch_size,1,1)))\n",
        "  grouped_xyz = tf.reshape(xyz,(batch_size, 1, nsample, 3))\n",
        "\n",
        "  if points is not None:\n",
        "     if use_xyz:\n",
        "        new_points = tf.concat([xyz,points])\n",
        "     else:\n",
        "        new_points = points\n",
        "     new_points = tf.expand_dims(new_points,1)\n",
        "  else:\n",
        "     new_points = grouped_xyz\n",
        "  return new_xyz, new_points,idx,grouped_xyz            \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKG1VAbdfdZ9"
      },
      "outputs": [],
      "source": [
        "class Conv2d(Layer):\n",
        "  def __init__(self,filters,strides=[1,1],activation=tf.nn.relu,padding='valid',initializer='glorot_normal',bn=False):\n",
        "    super(Conv2d,self).__init__\n",
        "\n",
        "    self.filters = filters\n",
        "    self.strides = strides\n",
        "    self.activation = activation\n",
        "    self.padding = padding \n",
        "    self.initializer = initializer\n",
        "    self.bn = bn\n",
        "\n",
        "  def build(self,input_shape):\n",
        "\n",
        "    self.w = self.add_weight(shape=(1, 1, input_shape[-1], self.filters),initializer=self.initializer,trainable=True,name='pnet_conv')\n",
        "\n",
        "    if self.bn: self.bn_layer = BatchNormalization()\n",
        "    \n",
        "    super(Conv2d, self).build(input_shape)\n",
        "\n",
        "  def call(self,inputs,training=True):\n",
        "\n",
        "    points = tf.nn.conv2d(inputs,filters=self.w,strides=self.strides,padding=self.padding)\n",
        "\n",
        "    if self.bn: points = self.bn_layers(points,training=training)\n",
        "    if self.activation: points = self.activation(points)\n",
        "\n",
        "    return points  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8icPtD8-bQ44"
      },
      "outputs": [],
      "source": [
        "class Pointnet_SA(Layer):\n",
        "  def __init__(\n",
        "\t\t  self, npoint, radius, nsample, mlp, group_all=False, knn=False, use_xyz=True, activation=tf.nn.relu, bn=False):\n",
        "      super(Pointnet_SA, self).__init__()\n",
        "\n",
        "      self.npoint = npoint\n",
        "      self.radius = radius\n",
        "      self.nsample = nsample\n",
        "      self.mlp = mlp\n",
        "      self.group_all = group_all\n",
        "      self.knn = knn\n",
        "      self.use_xyz = use_xyz \n",
        "      self.activation = activation\n",
        "      self.bn = bn\n",
        "\n",
        "      self.mlp_list = []\n",
        "\n",
        "  def build(self,input_shape):\n",
        "     for i,n_filters in enumerate(self.mlp):\n",
        "        self.mlp_list.append(Conv2d(n_filters,activation = self.activation,bn = self.bn))\n",
        "\n",
        "     super(Pointnet_SA,self).build(input_shape)\n",
        "\n",
        "  def call(self,xyz,points,training=True):\n",
        "    if points is not None:\n",
        "      if len(points.shape) < 3:\n",
        "        points = tf.expand_dims(points,axis=0)\n",
        "\n",
        "    if self.group_all:\n",
        "       nsample = xyz.get_shape()[1]\n",
        "       new_xyz, new_points,idx,grouped_xyz = sample_and_group_all(xyz,points,self.use_xyz)\n",
        "\n",
        "    else:\n",
        "      new_xyz, new_points,idx, grouped_xyz = sample_and_group(\n",
        "                                             self.npoint,\n",
        "                                             self.radius,\n",
        "                                             self.nsample,\n",
        "                                             xyz,\n",
        "                                             points,\n",
        "                                             self.knn,\n",
        "                                             use_xyz = self.use_xyz)\n",
        "\n",
        "    for i, mlp_layer in enumerate(self.mlp_list):\n",
        "        new_points = mlp_layer(new_points,training=training)\n",
        "\n",
        "    new_points = tf.math.reduce_max(new_points,axis=2,keepdims=True)\n",
        "\n",
        "    return new_xyz,tf.squeeze(new_points)                        \n",
        "\n",
        "\t\t  \n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\t   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXmIqb8zIZWl"
      },
      "outputs": [],
      "source": [
        "class Pointnet_SA_MSG(Layer):\n",
        "\n",
        "  def __init__(self,npoint,radius_list,nsample_list,mlp,use_xyz=True,activation=tf.nn.relu,bn=False):\n",
        "      super(Pointnet_SA_MSG,self).__init__()\n",
        "      self.npoint = npoint\n",
        "      self.radius_list = radius_list\n",
        "      self.nsample_list = nsample_list\n",
        "      self.mlp = mlp\n",
        "      self.use_xyz = use_xyz\n",
        "      self.activation = activation\n",
        "      self.bn = bn\n",
        "\n",
        "      self.mlp_list = []\n",
        "\n",
        "  def build(self,input_shape):\n",
        "\n",
        "    for i in range(len(self.radius_list)):\n",
        "        temp_list = []\n",
        "        for i , n_filters in enumerate(self.mlp[i]):\n",
        "            temp_list.append(Conv2d(n_filters,activation=self.activation,bn=self.bn))\n",
        "        self.mlp_list.append(temp_list)\n",
        "    super(Pointnet_SA_MSG,self).build(input_shape)\n",
        "\n",
        "  def Call(self,xyz,points, training=True):\n",
        "    print(xyz)\n",
        "    print(points)\n",
        "    if points is not None:\n",
        "      if len(points.shape) < 3:\n",
        "         points = tf.expand_dims(points,axis = 0)\n",
        "\n",
        "    new_xyz = index_points(xyz,farthest_point_sample(self.npoint,xyz))\n",
        "\n",
        "    new_point_list = []\n",
        "\n",
        "    for i in range(len(self.radius_list)):\n",
        "        radius = self.radius_list[i]\n",
        "        nsample = self.nsample[i]\n",
        "        idx,pts_cnt = query_ball_point(radius,nsample,xyz,new_xyz)\n",
        "        grouped_xyz = index_points(xyz,idx)\n",
        "        grouped_xyz -= tf.tile(tf.expand_dims(new_xyz, 2), [1,1,nsample,1])\n",
        "\n",
        "        if points is not None:\n",
        "          grouped_points = index_points(points,idx)\n",
        "          if self.use_xyz:\n",
        "            grouped_points = tf.concat([grouped_points,grouped_xyz],axis=-1)\n",
        "        else:\n",
        "          grouped_points = grouped_xyz\n",
        "\n",
        "        for i,mlp_layer in enumerate(self.mlp_list[i]):\n",
        "          grouped_points = mlp_layer(grouped_points,trainig = training)\n",
        "\n",
        "        new_points = tf.math.reduce_max(grouped_points,axis = 2)\n",
        "        new_point_list.append(new_points)  \n",
        "\n",
        "    new_points_concat = tf.concat(new_point_list,axis = -1)\n",
        "\n",
        "    return new_xyz, new_points_concat            \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5eUr-e6XXYe"
      },
      "outputs": [],
      "source": [
        "class CLS_MSG_Model(Model):\n",
        "\n",
        "   def __init__(self, batch_size, num_classes, bn=False, activation=tf.nn.relu):\n",
        "     super(CLS_MSG_Model, self).__init__()\n",
        "\n",
        "     self.activation = activation\n",
        "     self.batch_size = batch_size\n",
        "     self.num_classes = num_classes\n",
        "     self.bn = bn\n",
        "     self.keep_prob = 0.4\n",
        "\n",
        "     self.kernel_initializer = 'glorot_normal'\n",
        "     self.kernel_regularizer = None\n",
        "\n",
        "     self.init_network()\n",
        "\n",
        "   def init_network(self):\n",
        "\n",
        "     self.layer1 = Pointnet_SA_MSG(\n",
        "                npoint=1024,\n",
        "                radius_list=[0.1,0.2,0.4],\n",
        "                nsample_list=[16,32,128],\n",
        "                mlp=[[32,32,64], [64,64,128], [64,96,128]],\n",
        "                activation=self.activation,\n",
        "                bn = self.bn\n",
        "                )\n",
        "     self.layer2 = Pointnet_SA_MSG(\n",
        "                npoint=512,\n",
        "                radius_list=[0.2,0.4,0.8],\n",
        "                nsample_list=[32,64,128],\n",
        "                mlp=[[64,64,128], [128,128,256], [128,128,256]],\n",
        "                activation=self.activation,\n",
        "                bn = self.bn\n",
        "                )\n",
        "     self.layer3 = Pointnet_SA(npoint=None,\n",
        "                               radius=None,\n",
        "                               nsample=None,\n",
        "                               mlp=[256, 512, 1024],\n",
        "                               group_all=True,\n",
        "                               activation=self.activation,\n",
        "                               bn = self.bn)\n",
        "     self.dense1 = Dense(512, activation=self.activation)\n",
        "     self.dropout1 = Dropout(self.keep_prob)\n",
        "     self.dense2 = Dense(128, activation=self.activation)\n",
        "     self.dense2 = Dense(128, activation=self.activation)\n",
        "\n",
        "   def forward_pass(self,input,training):\n",
        "\n",
        "    xyz, points = self.layer1(input, None, training=training)\n",
        "    xyz, points = self.layer2(xyz, points, training=training)\n",
        "    xyz, points = self.layer3(xyz, points, training=training)\n",
        "\n",
        "    net = tf.reshape(points, (self.batch_size, -1))\n",
        "    net = self.dense1(net)\n",
        "    net = self.dropout1(net)\n",
        "    net = self.dense2(net)\n",
        "    net = self.dropout2(net)\n",
        "    pred = self.dense3(net)\n",
        "\n",
        "    return pred\n",
        "\n",
        "   def train_step(self,input):\n",
        "     with tf.GradientTape() as tape:\n",
        "         pred = self.forward_pass(input[0], True)\n",
        "         loss = self.compiled_loss(input[1], pred)\n",
        "     gradients = tape.gradient(loss, self.trainable_variables)\n",
        "     self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "     self.compiled_metrics.update_state(input[1], pred)\n",
        "     return {m.name: m.result() for m in self.metrics}  \n",
        "\n",
        "   def test_step(self,input):\n",
        "     pred = self.forward_pass(input[0], False)\n",
        "     loss = self.compiled_loss(input[1], pred)\n",
        "\n",
        "     self.compiled_metrics.update_state(input[1], pred)\n",
        "     return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "   def call(self,input,training=False):\n",
        "      return self.forward_pass(input, training)        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcBmS3C981Mj"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "msg = True\n",
        "bn = False "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7pyIyCiUSHB",
        "outputId": "3823ba13-757e-48fd-a18a-f47c989105d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<KerasTensor: shape=() dtype=int32 inferred_value=[None] (created by layer 'tf.compat.v1.size')>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#input_shape = (batch_size,8192,3)\n",
        "#inputs = tf.keras.Input(input_shape)\n",
        "#tf.size(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "SQig8uE_NnAS",
        "outputId": "e8009138-e3ac-4242-84e6-3501d450c047"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-6f6f2daafd9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#model.build(input_shape=(batch_size,8192,3))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-83fb525e1d24>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input, training)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-83fb525e1d24>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(self, input, training)\u001b[0m\n\u001b[1;32m     47\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mxyz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mxyz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxyz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mxyz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxyz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"cls_msg__model_3\" (type CLS_MSG_Model).\n\ntoo many values to unpack (expected 2)\n\nCall arguments received by layer \"cls_msg__model_3\" (type CLS_MSG_Model):\n  • input=tf.Tensor(shape=(2048, 3), dtype=float32)\n  • training=False"
          ]
        }
      ],
      "source": [
        "model = CLS_MSG_Model(batch_size,classes,bn)\n",
        "#inputs = tf.keras.Input(shape=(points, 3))\n",
        "#model = tf.keras.Model(inputs,model)\n",
        "\n",
        "#model.build(input_shape=(batch_size,8192,3))\n",
        "new = model(train_points[0])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZLYvbRn-NzE"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVFynx6V-aj1"
      },
      "outputs": [],
      "source": [
        "model.fit(train_dataset,epochs=50,validation_data=test_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1e1YyoZltHnc_4fvnm5kYIT00CpcHldSk",
      "authorship_tag": "ABX9TyNz2xI9c0FLrVYvoF4lPWEK",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}